{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "60e8e917ea5b4762b1cc8a489aa30eb6",
        "deepnote_cell_type": "markdown",
        "tags": [],
        "id": "AsB3xj6tlPUQ"
      },
      "source": [
        "# Home assignment\n",
        "\n",
        "* Author: Romain Tavenard (@rtavenar)\n",
        "* License: CC-BY-NC-SA\n",
        "\n",
        "A home assignment from a course on Deep Learning at EDHEC.\n",
        "\n",
        "## Problem statement\n",
        "\n",
        "The dataset we are interested in here is called \"CIFAR10\". It is described [in this page](https://keras.io/api/datasets/cifar10/).\n",
        "\n",
        "You should load the data, **select only 5,000 samples out of the total 50,000 ones**, and preprocess it if needed.\n",
        "You should compare several candidate neural network architectures, and make a decision about which is best for the task at hand.\n",
        "You should be explicit about the indicator(s) you base your decision on.\n",
        "\n",
        "Finally, as a bonus, you could try to evaluate whether it is better to:\n",
        "* train a model from scratch on this dataset alone ;\n",
        "* use a large model that was pre-trained on ImageNet ;\n",
        "* pre-train a model on another dataset called [CIFAR100](https://keras.io/api/datasets/cifar100/) and fine-tune it on CIFAR10.\n",
        "\n",
        "## Deadline\n",
        "\n",
        "Deadline for this home assignment is February 28th, 2025.\n",
        "You should use the link on moodle to hand in your assignment.\n",
        "A single ipynb file should be provided,\n",
        "with execution traces.\n",
        "This assignment is to be done **by groups of three, at most** and names of all students should be included in the file name.\n",
        "\n",
        "## Data loading\n",
        "\n",
        "You can use the dedicated `keras` utility to load this dataset: <https://keras.io/api/datasets/cifar10/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data"
      ],
      "metadata": {
        "id": "1S2i4yc5n8wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "2BdoCJoDs93j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
      ],
      "metadata": {
        "id": "Zl-CCQVotquO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shaping the dataset CIFAR10"
      ],
      "metadata": {
        "id": "joGunAOquYWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "assert x_train.shape == (50000, 32, 32, 3)\n",
        "assert x_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ],
      "metadata": {
        "id": "Rz8joqavtmdW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_samples = 4500\n",
        "num_test_samples = 500\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "train_indices = np.random.choice(x_train.shape[0], num_train_samples, replace=False)\n",
        "test_indices = np.random.choice(x_test.shape[0], num_test_samples, replace=False)\n",
        "\n",
        "x_train = x_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "x_test = x_test[test_indices]\n",
        "y_test = y_test[test_indices]\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "metadata": {
        "id": "rP7RE26bu6ra"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KauSfmwLu36g",
        "outputId": "451ee1ca-1fa3-42af-82ae-e486cf9a3762"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4500, 32, 32, 3)\n",
            "(500, 32, 32, 3)\n",
            "(4500, 10)\n",
            "(500, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keras CIFAR100"
      ],
      "metadata": {
        "id": "fKOvog8UBI15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shaping the dataset CIFAR100"
      ],
      "metadata": {
        "id": "oQ20kVz1BWcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_100, y_train_100), (x_test_100, y_test_100) = keras.datasets.cifar100.load_data()\n",
        "assert x_train_100.shape == (50000, 32, 32, 3)\n",
        "assert x_test_100.shape == (10000, 32, 32, 3)\n",
        "assert y_train_100.shape == (50000, 1)\n",
        "assert y_test_100.shape == (10000, 1)"
      ],
      "metadata": {
        "id": "WLrNE5zQBZVZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_100 = to_categorical(y_train_100)\n",
        "y_test_100 = to_categorical(y_test_100)\n",
        "\n",
        "x_train_100 = x_train_100 / 255\n",
        "x_test_100 = x_test_100 / 255"
      ],
      "metadata": {
        "id": "8H059kQyDa8u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "zzwfNMZmlmHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple CNN Models"
      ],
      "metadata": {
        "id": "BWHvu10xlvEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model"
      ],
      "metadata": {
        "id": "c-bPxQ8Vmx0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model():\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(32, 32, 3)),\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=2),\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hPKUkakJlnMl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model Including Data Augmentation"
      ],
      "metadata": {
        "id": "O76VRnapmZrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augmented_baseline_model():\n",
        "    data_augmentation = Sequential([\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(0.1),\n",
        "        RandomZoom(0.2),\n",
        "    ])\n",
        "\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(32, 32, 3)),\n",
        "        data_augmentation,\n",
        "        Conv2D(filters=16, kernel_size=(2, 2), activation='relu'),\n",
        "        MaxPooling2D(pool_size=4),\n",
        "        Conv2D(filters=16, kernel_size=(2, 2), activation='relu'),\n",
        "        MaxPooling2D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KfI5oxjvmYhE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Le Net Model"
      ],
      "metadata": {
        "id": "xUjwNbwtq14h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lenet_model():\n",
        "    \"\"\"LeNet architecture implementation\"\"\"\n",
        "    model = Sequential([\n",
        "        # First convolutional layer\n",
        "        Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        # Second convolutional layer\n",
        "        Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        # Fully connected layers\n",
        "        Flatten(),\n",
        "        Dense(120, activation='relu'),\n",
        "        Dense(84, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])"
      ],
      "metadata": {
        "id": "zCH2Qdscq37w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deeper CNNs"
      ],
      "metadata": {
        "id": "tq6FXzOJm2fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic 3 Block Deep CNN"
      ],
      "metadata": {
        "id": "8ol6uGI3p9VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_cnn_model():\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(32, 32, 3)),\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])"
      ],
      "metadata": {
        "id": "Jv-UayNqmrdt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic 3 Block Deep CNN with regularization"
      ],
      "metadata": {
        "id": "uUwbQS0wqBq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with Global Average Pooling and L2 regularization\n",
        "def regularized_deep_cnn():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu',\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                               input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu',\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uY4APkkop8pk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning Models"
      ],
      "metadata": {
        "id": "dhdza0XuneZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50 Imagenet, only tuning classification layers"
      ],
      "metadata": {
        "id": "7iwgoqhknj5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet50_false_model(x_train):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=x_train[0].shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7GcdY8gvm0YR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50 Imagenet, tuning all layers"
      ],
      "metadata": {
        "id": "d-3k7OeVoGnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet50_true_model(x_train):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=x_train[0].shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "tPKnC5ucndlP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR100 Pretraining, CIFAR10 Fine Tuning"
      ],
      "metadata": {
        "id": "HtAZy0GToNnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_and_finetune(x_train_100, y_train_100, x_test_100, y_test_100, x_train, y_train, x_test, y_test):\n",
        "    # Early stopping callback\n",
        "    cb = EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Pretrain on CIFAR-100\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(32, 32, 3)),\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=2),\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(100, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train_100, y_train_100, epochs=100000, validation_data=(x_test_100, y_test_100), callbacks=[cb])\n",
        "    model.save('cifar100_pretrained_model.h5')\n",
        "\n",
        "    # Fine-tune on CIFAR-10\n",
        "    model_cifar100 = load_model('cifar100_pretrained_model.h5')\n",
        "    feature_extractor = model_cifar100.layers[-3].output\n",
        "    output = Dense(10, activation='softmax')(feature_extractor)\n",
        "\n",
        "    model_cifar10 = Model(inputs=model_cifar100.input, outputs=output)\n",
        "    for layer in model_cifar100.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model_cifar10.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model_cifar10"
      ],
      "metadata": {
        "id": "2h8hP4ccob_M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep NN, CIFAR100 Pretraining, CIFAR10 Fine Tuning"
      ],
      "metadata": {
        "id": "YsNKoDl5qpos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_and_finetune_deep(x_train_100, y_train_100, x_test_100, y_test_100, x_train, y_train, x_test, y_test):\n",
        "    # Early stopping callback\n",
        "    cb = EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Pretrain on CIFAR-100 with deeper architecture\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(32, 32, 3)),\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(100, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train_100, y_train_100, epochs=100000, validation_data=(x_test_100, y_test_100), callbacks=[cb])\n",
        "    model.save('cifar100_pretrained_model2.h5')\n",
        "\n",
        "    # Fine-tune on CIFAR-10\n",
        "    model_cifar100 = load_model('cifar100_pretrained_model2.h5')\n",
        "    feature_extractor = model_cifar100.layers[-3].output\n",
        "    output = Dense(10, activation='softmax')(feature_extractor)\n",
        "\n",
        "    model_cifar10 = Model(inputs=model_cifar100.input, outputs=output)\n",
        "    for layer in model_cifar100.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model_cifar10.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model_cifar10.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "    return model_cifar10"
      ],
      "metadata": {
        "id": "EOEeCVBjqoon"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "IUxYCRXbri6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function"
      ],
      "metadata": {
        "id": "rHasXWrDsVuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(models, x_train, y_train, x_val, y_val):\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        history = model.fit(\n",
        "            x_train, y_train, batch_size=64, epochs=100,\n",
        "            validation_data=(x_val, y_val),\n",
        "            callbacks=callbacks, verbose=1\n",
        "        )\n",
        "        results[name] = {'history': history}\n",
        "    return results"
      ],
      "metadata": {
        "id": "-gkeID1ssiu1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "4DeHS9k0sjfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    \"Baseline CNN\": baseline_model(),\n",
        "    \"Augmented CNN\": augmented_baseline_model(),\n",
        "    \"Le Net Model\": lenet_model(),\n",
        "    \"Deeper CNN\": deep_cnn_model(),\n",
        "    \"Regularized Deeper CNN\": regularized_deep_cnn(),\n",
        "    \"Res Net 50 False\": resnet50_false_model(x_train),\n",
        "    \"Res Net 50 True\": resnet50_true_model(x_train),\n",
        "}\n",
        "\n",
        "# Run the training, plotting, and evaluation\n",
        "results = train_models(models, x_train, y_train, x_val, y_val)\n",
        "\n"
      ],
      "metadata": {
        "id": "76kwLX-4rmPd",
        "outputId": "c511449c-6adc-4e42-bf4b-96215c4098c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'augmented_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e74364005e18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m models = {\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"Baseline CNN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m\"Augmented CNN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maugmented_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"Deep CNN\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdeep_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"Transfer Learning\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransfer_learning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'augmented_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "w03xvfuOr4YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "mBAslI48r-08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function"
      ],
      "metadata": {
        "id": "DR-LsU1ysss2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(results):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for name, result in results.items():\n",
        "        plt.plot(result['history'].history['accuracy'], label=f'{name} (Train)')\n",
        "        plt.plot(result['history'].history['val_accuracy'], label=f'{name} (Val)')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for name, result in results.items():\n",
        "        plt.plot(result['history'].history['loss'], label=f'{name} (Train)')\n",
        "        plt.plot(result['history'].history['val_loss'], label=f'{name} (Val)')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NBvqP3V6r2qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plots"
      ],
      "metadata": {
        "id": "N7fqVjyusvjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(results)\n"
      ],
      "metadata": {
        "id": "ABaWgil1sxHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "58ayUi3hsAW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function"
      ],
      "metadata": {
        "id": "fI5458p7s02v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(models, results, x_test, y_test):\n",
        "    print(\"\\nModel Evaluation:\")\n",
        "    best_model_name, best_acc = None, 0\n",
        "\n",
        "    for name, model in models.items():\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        results[name]['test_acc'] = test_acc\n",
        "        print(f\"{name} - Test Accuracy: {test_acc:.4f}\")\n",
        "        if test_acc > best_acc:\n",
        "            best_model_name, best_acc = name, test_acc\n",
        "\n",
        "    print(f\"\\nBest model: {best_model_name} with test accuracy of {best_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "n2-89gVhsBHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tests"
      ],
      "metadata": {
        "id": "PWonj_Tks2fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_models(models, results, x_test, y_test)"
      ],
      "metadata": {
        "id": "j5dUdIQGs3m0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "da3f2328c5444c2a931e21e0c8570d48",
    "kernelspec": {
      "display_name": "py38_data",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "25f9a3951446179f6c2016b22a60b44495fe90f43bda7f3caedfe2c1a9cd31f9"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}